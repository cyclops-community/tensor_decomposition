{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import scipy.linalg as sla\n",
    "import scipy.sparse.linalg as spsalg\n",
    "import tensors.synthetic_tensors as synthetic_tensors\n",
    "import backend.numpy_ext as tenpy\n",
    "from CPD.common_kernels import get_residual, compute_lin_sys\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive NLS Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian3(A):\n",
    "    J = np.zeros((Q,order*s*R))\n",
    "    for i in range(order):\n",
    "        offset1 = i*s*R\n",
    "        for j in range(R):\n",
    "            offset2 = j*s\n",
    "            start = offset1+offset2\n",
    "            end = start + s\n",
    "            if i==0:\n",
    "                J[:,start:end] = np.kron(np.identity(s),np.kron(A[1][:,j],A[2][:,j])).T\n",
    "            elif i==1:\n",
    "                J[:,start:end] = np.kron(A[0][:,j],np.kron(np.identity(s),A[2][:,j])).T\n",
    "            elif i==2:\n",
    "                J[:,start:end] = np.kron(A[0][:,j],np.kron(A[1][:,j],np.identity(s))).T\n",
    "    return J\n",
    "\n",
    "def F(T,A):\n",
    "    f = (T - np.einsum(\"ir,jr,kr->ijk\",A[0],A[1],A[2])).reshape(-1)\n",
    "    return f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(T,A):\n",
    "    g = np.zeros(order*s*R)\n",
    "    G = []\n",
    "    \n",
    "    TC = tenpy.einsum(\"ijk,ka->ija\",T,A[2])\n",
    "    M1 = tenpy.einsum(\"ija,ja->ia\",TC,A[1])\n",
    "    G1 = -1*M1 + np.dot(A[0],compute_lin_sys(tenpy,A[1],A[2],0))\n",
    "    G.append(G1)\n",
    "    \n",
    "    M2 = tenpy.einsum(\"ija,ia->ja\",TC,A[0])\n",
    "    G2 = -1*M2 + np.dot(A[1],compute_lin_sys(tenpy,A[0],A[2],0))\n",
    "    G.append(G2)\n",
    "    \n",
    "    M3 = tenpy.einsum(\"ijk,ia,ja->ka\",T,A[0],A[1])\n",
    "    G3 = -1*M3 + np.dot(A[2],compute_lin_sys(tenpy,A[0],A[1],0))\n",
    "    G.append(G3)\n",
    "    \n",
    "    for i in range(order):\n",
    "        offset1 = i*s*R\n",
    "        for j in range(R):\n",
    "            offset2 = j*s\n",
    "            start = offset1 + offset2\n",
    "            end = start + s\n",
    "            g[start:end] = G[i][:,j]\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_A(A):\n",
    "    a = np.zeros(order*R*s)\n",
    "    for i in range(order):\n",
    "        offset1 = i*s*R\n",
    "        for j in range(R):\n",
    "            offset2 = j*s\n",
    "            start = offset1+offset2\n",
    "            end = start+s\n",
    "            a[start:end] = A[i][:,j]\n",
    "    return a\n",
    "\n",
    "def form_dA(A,x):\n",
    "    dA = []\n",
    "    for i in range(len(A)):\n",
    "        dA.append(np.zeros(A[i].shape))\n",
    "    for i in range(order):\n",
    "        offset1 = i*s*R\n",
    "        for j in range(R):\n",
    "            offset2 = j*s\n",
    "            start = offset1+offset2\n",
    "            end = start+s\n",
    "            dA[i][:,j] += x[start:end]\n",
    "    return dA\n",
    "            \n",
    "\n",
    "def update_A(A,x,alpha=1):\n",
    "    for i in range(order):\n",
    "        offset1 = i*s*R\n",
    "        for j in range(R):\n",
    "            offset2 = j*s\n",
    "            start = offset1+offset2\n",
    "            end = start+s\n",
    "            A[i][:,j] += alpha*x[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast NLS Implementation with Block Matrix-Vector Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast Hessian approximation by Gauss-Newton\n",
    "def compute_coeff(G,n1,r1,n2,r2):\n",
    "    return np.prod([G[i][r1,r2] for i in range(len(G)) if i!=n1 and i!=n2])\n",
    "        \n",
    "def compute_block(A,G,n1,r1,n2,r2):\n",
    "    if n1 == n2:\n",
    "        return compute_coeff(G,n1,r1,n2,r2)*np.identity(A[0].shape[0])\n",
    "    else:\n",
    "        return compute_coeff(G,n1,r1,n2,r2)*np.outer(A[n1][:,r2],A[n2][:,r1])\n",
    "\n",
    "def fast_hessian3(A):\n",
    "    G1 = A[0].T.dot(A[0])\n",
    "    G2 = A[1].T.dot(A[1])\n",
    "    G3 = A[2].T.dot(A[2])\n",
    "    G = [G1,G2,G3]\n",
    "    N = order*s*R\n",
    "    hessian = np.zeros((N,N))\n",
    "    \n",
    "    for n1 in range(order):\n",
    "        for r1 in range(R):\n",
    "            startv = n1*R*s + r1*s\n",
    "            endv = startv + s\n",
    "            for n2 in range(order):\n",
    "                for r2 in range(R):\n",
    "                    starth = n2*R*s + r2*s\n",
    "                    endh = starth + s\n",
    "                    hessian[startv:endv,starth:endh] = compute_block(A,G,n1,r1,n2,r2)\n",
    "    return hessian\n",
    "\n",
    "def compute_result_block(A,G,n1,r1,n2,r2,x):\n",
    "    if n1==n2:\n",
    "        return compute_coeff(G,n1,r1,n2,r2)*x\n",
    "    else:\n",
    "        s = compute_coeff(G,n1,r1,n2,r2)*np.inner(A[n2][:,r1],x)\n",
    "        return s*A[n1][:,r2]\n",
    "\n",
    "def fast_hessian3_mult(A,x,regu=1):\n",
    "    ret = regu*x\n",
    "    G = []\n",
    "    for i in range(len(A)):\n",
    "        G.append(A[i].T.dot(A[i]))\n",
    "    \n",
    "    for n1 in range(order):\n",
    "        for r1 in range(R):\n",
    "            startv = n1*R*s + r1*s\n",
    "            endv = startv + s\n",
    "            for n2 in range(order):\n",
    "                for r2 in range(R):\n",
    "                    starth = n2*R*s + r2*s\n",
    "                    endh = starth + s\n",
    "                    ret[startv:endv] += compute_result_block(A,G,n1,r1,n2,r2,x[starth:endh])\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradient(A,x,b,tol=1e-4,formula=\"PR\"):\n",
    "    tol = np.max([tol,tol*la.norm(b)])\n",
    "    r = b - A.dot(x)\n",
    "    if la.norm(r)<tol:\n",
    "        return x,0\n",
    "    p = r\n",
    "    counter = 0\n",
    "    while True:\n",
    "        alpha = np.inner(r,r)/np.inner(p,A.dot(p))\n",
    "        x += alpha*p\n",
    "        r_new = r - alpha*A.dot(p)\n",
    "        if la.norm(r_new)<tol:\n",
    "            break\n",
    "        if formula == \"PR\":\n",
    "            beta = np.inner(r_new,r_new-r)/np.inner(r,r)\n",
    "        elif formula == \"HS\":\n",
    "            y = r_new - r\n",
    "            beta = np.inner(r_new,y)/np.inner(y,p)\n",
    "        else:\n",
    "            beta = np.inner(r_new,r_new)/np.inner(r,r)\n",
    "        p = r_new + beta*p\n",
    "        r = r_new\n",
    "        counter += 1\n",
    "    print(\"conjugate gradient took \",counter,\" iteration(s).\")\n",
    "    return x,counter\n",
    "\n",
    "def preconditioned_conjugate_gradient(A,x,b,M,tol=1e-4,formula=\"PR\"):\n",
    "    tol = np.max([tol,tol*la.norm(b)])\n",
    "    r = b - A.dot(x)\n",
    "    counter = 0\n",
    "    if la.norm(r)<tol:\n",
    "        return x,counter\n",
    "    z = M.dot(r)\n",
    "    p = z\n",
    "    while True:\n",
    "        alpha = np.inner(r,z)/np.inner(p,A.dot(p))\n",
    "        x += alpha*p\n",
    "        r_new = r - alpha*A.dot(p)\n",
    "        if la.norm(r_new)<tol: ## need to add max iteration\n",
    "            break\n",
    "        z_new = M.dot(r_new)\n",
    "        if formula == \"PR\":\n",
    "            beta = np.inner(z_new,r_new-r)/np.inner(z,r)\n",
    "        elif formula == \"HS\":\n",
    "            y=z_new-z\n",
    "            beta = np.inner(z_new,y)/np.inner(y,p)\n",
    "        else:\n",
    "            beta = np.inner(z_new,r_new)/np.inner(z,r)\n",
    "        p = z_new + beta*p\n",
    "        r = r_new\n",
    "        z = z_new\n",
    "        counter += 1\n",
    "    return x,counter\n",
    "\n",
    "def preconditioned_conjugate_gradient2(A,x,b,M,tol=1e-4,formula=\"PR\"):\n",
    "    tol = np.max([tol,tol*la.norm(b)])\n",
    "    r = b - A.dot(x)\n",
    "    counter = 0\n",
    "    if la.norm(r)<tol:\n",
    "        return x,counter\n",
    "    z = naive_block_GS(A,3,R,r) #M.dot(r)\n",
    "    p = z\n",
    "    while True:\n",
    "        alpha = np.inner(r,z)/np.inner(p,A.dot(p))\n",
    "        x += alpha*p\n",
    "        r_new = r - alpha*A.dot(p)\n",
    "        if la.norm(r_new)<tol: ## need to add max iteration\n",
    "            break\n",
    "        z_new = naive_block_GS(A,3,R,r_new) #M.dot(r_new)\n",
    "        if formula == \"PR\":\n",
    "            beta = np.inner(z_new,r_new-r)/np.inner(z,r)\n",
    "        else:\n",
    "            beta = np.inner(z_new,r_new)/np.inner(z,r)\n",
    "        p = z_new + beta*p\n",
    "        r = r_new\n",
    "        z = z_new\n",
    "        counter += 1\n",
    "    print(\"conjugate gradient took \",counter,\" iteration(s).\")\n",
    "    return x,counter\n",
    "\n",
    "\n",
    "def fast_conjugate_gradient():\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetric_solve(H,x):\n",
    "    L = la.cholesky(H)\n",
    "    Y = sla.solve_triangular(L,x,trans=0,lower=True)\n",
    "    X = sla.solve_triangular(L,Y,trans=1,lower=True)\n",
    "    return X\n",
    "\n",
    "def backward_symmetric_solve(H,x):\n",
    "    X = symmetric_solve(H,x.T)\n",
    "    return X.T\n",
    "\n",
    "def naive_block_Jacobi(H,order,stride):\n",
    "    M = np.zeros(H.shape)\n",
    "    for i in range(order):\n",
    "        start = i*stride\n",
    "        end = start + stride\n",
    "        M[start:end,start:end] = H[start:end,start:end]\n",
    "    L = la.cholesky(M)\n",
    "    Y = sla.solve_triangular(L,np.identity(H.shape[0]),trans=0,lower=True)\n",
    "    X = sla.solve_triangular(L,Y,trans=1,lower=True)\n",
    "    return X\n",
    "\n",
    "def naive_block_GS(H,order,stride,x):\n",
    "    ret = np.copy(x)\n",
    "    for n in range(order):\n",
    "        for p in range(n+1):\n",
    "            startv = n*stride\n",
    "            endv = startv + stride\n",
    "            starth = p*stride\n",
    "            endh = starth + stride\n",
    "            M = H[startv:endv,starth:endh]\n",
    "            if n==p:\n",
    "                ret[startv:endv] = symmetric_solve(M,x[startv:endv])\n",
    "            else:\n",
    "                ret[startv:endv] -= M.dot(x[startv:endv])\n",
    "    return ret\n",
    "\n",
    "def naive_block_SGS(H,order,stride,x):\n",
    "    ret = np.copy(x)\n",
    "    for n in range(order):\n",
    "        for p in range(n+1):\n",
    "            startv = n*stride\n",
    "            endv = startv + stride\n",
    "            starth = p*stride\n",
    "            endh = starth + stride\n",
    "            M = H[startv:endv,starth:endh]\n",
    "            if n==p:\n",
    "                ret[startv:endv] = symmetric_solve(M,x[startv:endv])\n",
    "            else:\n",
    "                ret[startv:endv] -= M.dot(x[startv:endv])\n",
    "    for n in range(order):\n",
    "        startv = n*stride\n",
    "        endv = startv + stride\n",
    "        M = H[startv:endv,startv:endv]\n",
    "        ret[startv:endv] = M.dot(ret[startv:endv])\n",
    "    for n in range(order,0,-1):\n",
    "        for p in range(order,n-1,-1):\n",
    "            startv = (n-1)*stride\n",
    "            endv = startv + stride\n",
    "            starth = (p-1)*stride\n",
    "            endh = starth + stride\n",
    "            M = H[startv:endv,starth:endh]\n",
    "            if n==p:\n",
    "                ret[startv:endv] = symmetric_solve(M,ret[startv:endv])\n",
    "            else:\n",
    "                ret[startv:endv] -= M.dot(ret[startv:endv])\n",
    "    return ret\n",
    "\n",
    "def naive_SGS(H):\n",
    "    I = np.identity(H.shape[0])\n",
    "    Y = sla.solve_triangular(H,I,trans=0,lower=True)\n",
    "    D = np.diag(H)\n",
    "    Y = np.einsum(\"i,ij->ij\",D,Y)\n",
    "    Y = sla.solve_triangular(H,Y,trans=0,lower=False)\n",
    "    return Y\n",
    "\n",
    "def naive_GS(H):\n",
    "    I = np.identity(H.shape[0])\n",
    "    Y = sla.solve_triangular(H,I,trans=0,lower=True)\n",
    "    return Y\n",
    "\n",
    "def Jacobi_preconditioner(H):\n",
    "    s1 = np.sum(np.diag(H))\n",
    "    P = np.diag(1/np.diag(H))\n",
    "    s2 = np.sum(np.diag(P))\n",
    "    return P\n",
    "\n",
    "def block_LU(A,stride):\n",
    "    L = np.zeros(A.shape)\n",
    "    U = np.zeros(A.shape)\n",
    "    A11 = A[:stride,:stride]\n",
    "    \n",
    "    L[:stride,:stride] = np.identity(stride)\n",
    "    U[:stride,:] = A[:stride,:]\n",
    "    U12 = A[:stride,stride:]\n",
    "    L[stride:,:stride] = backward_symmetric_solve(A11,A[stride:,:stride])\n",
    "    L21 = L[stride:,:stride]\n",
    "    if A.shape[0] == stride:\n",
    "        return L,U\n",
    "    else:\n",
    "        L[stride:,stride:],U[stride:,stride:] = block_LU(A[stride:,stride:]-L21.dot(U12),stride)\n",
    "        return L,U\n",
    "\n",
    "def naive_block_diag_perturbative_preconditioner(H,order,stride):\n",
    "    A_inv = naive_block_Jacobi(H,order,stride)\n",
    "    _,s,_ = la.svd(A_inv.dot(H))\n",
    "    c = s[0]/2*1.01\n",
    "    A_inv /= c\n",
    "    I = np.identity(H.shape[0])\n",
    "    _,s,_ = la.svd(I-A_inv.dot(H))\n",
    "    print(\"norm of residual is \",s[0])\n",
    "    return A_inv.dot(2*I-H.dot(A_inv))\n",
    "\n",
    "def naive_diag_perturbative_preconditioner(H):\n",
    "    D_inv = np.diag(1/np.diag(H))\n",
    "    _,s,_ = la.svd(D_inv.dot(H))\n",
    "    print(\"L2 norm of D_inv*H is \",s[0])\n",
    "    c = s[0]/2*1.01\n",
    "    D_inv /= c\n",
    "    I = np.identity(H.shape[0])\n",
    "    return D_inv.dot(2*I - H.dot(D_inv))\n",
    "    \n",
    "def identity_like_iter_preconditioner(H):\n",
    "    Fnorm = la.norm(H,ord='fro')\n",
    "    print(\"Fnorm is \",Fnorm)\n",
    "    V0 = np.identity(H.shape[0])/Fnorm\n",
    "    return 2*V0 - H/Fnorm**2\n",
    "\n",
    "def identity_like_iter_preconditioner2(H):\n",
    "    #s = np.sum(np.diag(H))/la.norm(H)**2\n",
    "    #s = la.eigvalsh(H)[-1]/2+1\n",
    "    s = la.norm(H)\n",
    "    #print(\"sigma/2 is \",s)\n",
    "    #s2 = power_iteration(H,np.random.random(H.shape[0]))\n",
    "    #print(\"power iteration gives \",s2)\n",
    "    #print(\"power iteration estimate sigma/2 is \",s2/2)\n",
    "    I = np.identity(H.shape[0])\n",
    "    V0 = I/s\n",
    "    _,s,_=la.svd(I-V0.dot(H))\n",
    "    print(\"norm of residual is \",s[0])\n",
    "    return V0.dot(2*I-H.dot(V0))\n",
    "\n",
    "def transpose_iter_preconditioner(H):\n",
    "    d = np.max(np.sum(H,axis=1))\n",
    "    V0 = H/d**2\n",
    "    return 2*V0 + V0.dot(H.dot(V0))\n",
    "\n",
    "def power_iteration(H,x,iteration=1):\n",
    "    x = x/la.norm(x)\n",
    "    for i in range(iteration):\n",
    "        x = H.dot(x)\n",
    "        x = x/la.norm(x)\n",
    "    return np.inner(x,H.dot(x))\n",
    "\n",
    "def get_diag_block(H,order,stride):\n",
    "    M = np.zeros(H.shape)\n",
    "    for i in range(order):\n",
    "        start = i*stride\n",
    "        end = start + stride\n",
    "        M[start:end,start:end] = H[start:end,start:end]\n",
    "    return M\n",
    "\n",
    "def naive_diag_Neumann_preconditioner(H):\n",
    "    s = np.sum(np.diag(H))/la.norm(H)**2\n",
    "    D_inv = np.diag(1/np.diag(H))*s\n",
    "    H_hat = H-np.diag(np.diag(H))/s\n",
    "    return D_inv-D_inv.dot(H_hat.dot(D_inv))\n",
    "\n",
    "def naive_diag_Neumann_preconditioner2(H):\n",
    "    s = np.sum(np.diag(H))/la.norm(H)**2\n",
    "    X = np.identity(H.shape[0])*s\n",
    "    E = H-np.identity(H.shape[0])/s\n",
    "    return X-X.dot(E.dot(X))\n",
    "    \n",
    "def naive_block_diag_Neumann_preconditioner(H,order,stride):\n",
    "    s = np.sum(np.diag(H))/la.norm(H)**2\n",
    "    E = H-get_diag_block(H,order,stride)/s\n",
    "    D_inv = naive_block_Jacobi(H,order,stride)*s\n",
    "    return D_inv-D_inv.dot(E.dot(D_inv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.92741961 0.35452868]\n",
      " [0.08575361 0.91892579]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.92741961, 0.35452868, 0.08575361, 0.91892579])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten_Tensor(G,order,s,R):\n",
    "    g = np.zeros(order*s*R)\n",
    "    for i in range(order):\n",
    "        offset1 = i*s*R\n",
    "        for j in range(R):\n",
    "            offset2 = j*s\n",
    "            start = offset1 + offset2\n",
    "            end = start + s\n",
    "            g[start:end] = G[i][:,j]\n",
    "    return g\n",
    "\n",
    "def compute_number_of_variables(A):\n",
    "    a = 0\n",
    "    for matrix in A:\n",
    "        a += matrix.shape[0]*matrix.shape[1]\n",
    "    return a\n",
    "\n",
    "def flatten_Tensor2(G,order,s,R):\n",
    "    ssl = compute_sum_side_length(G)\n",
    "    g = np.zeros(ssl*R)\n",
    "    for i in range(order):\n",
    "        offset1 = i*s*R\n",
    "        for j in range(R):\n",
    "            offset2 = j*s\n",
    "            start = offset1 + offset2\n",
    "            end = start + s\n",
    "            g[start:end] = G[i][:,j]\n",
    "    return g\n",
    "\n",
    "A = np.random.random((2,2))\n",
    "print(A)\n",
    "np.reshape(A,-1,'C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line Search Correction Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_residual_tensor3(T,A):\n",
    "    assert(len(A)==3)\n",
    "    return T-np.einsum(\"ir,jr,kr->ijk\",A[0],A[1],A[2])\n",
    "\n",
    "def compute_alpha_numerator(R,dA):\n",
    "    return np.einsum(\"ijk,ir,jr,kr->\",R,dA[0],dA[1],dA[2])\n",
    "\n",
    "def compute_alpha_denominator(dA):\n",
    "    return np.einsum(\"ir,jr,kr,is,js,ks->\",dA[0],dA[1],dA[2],dA[0],dA[1],dA[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naive NLS implementation for order 3 CP decomposition\n",
    "order = 3\n",
    "s = 4\n",
    "R = 7\n",
    "sp_frac = 1\n",
    "iteration = 10\n",
    "\n",
    "[T,O] = synthetic_tensors.init_rand(tenpy,order,s,R,sp_frac)\n",
    "A = []\n",
    "dA = []\n",
    "for i in range(T.ndim):\n",
    "    A.append(tenpy.random((T.shape[i],R)))\n",
    "    dA.append(np.zeros((T.shape[i],R)))\n",
    "\n",
    "Q = s**order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Residual computation took', 0.00010848045349121094, 'seconds')\n",
      "Start residual is  4.727867857099142\n",
      "x shape is (84,)\n",
      "[ 0 ] iteration gradient norm is  15.119458148147414\n",
      "[ 0 ] conjugate gradient took  15  iteration(s).\n",
      "alpha is  11.337795606716382\n",
      "('Residual computation took', 7.82012939453125e-05, 'seconds')\n",
      "[ 0 ] iteration residual is  240.43408780336006\n",
      "[ 1 ] iteration gradient norm is  12062.58010301256\n",
      "[ 1 ] conjugate gradient took  48  iteration(s).\n",
      "alpha is  31.428778885394877\n",
      "('Residual computation took', 6.198883056640625e-05, 'seconds')\n",
      "[ 1 ] iteration residual is  300164.48420618003\n",
      "[ 2 ] iteration gradient norm is  1565089968.6320193\n",
      "[ 2 ] conjugate gradient took  2  iteration(s).\n",
      "alpha is  27.000036878245094\n",
      "('Residual computation took', 5.936622619628906e-05, 'seconds')\n",
      "[ 2 ] iteration residual is  300165365.78618413\n",
      "[ 3 ] iteration gradient norm is  156558798661403.94\n",
      "[ 3 ] conjugate gradient took  0  iteration(s).\n",
      "alpha is  26.9999996955858\n",
      "('Residual computation took', 0.00035119056701660156, 'seconds')\n",
      "[ 3 ] iteration residual is  300165359540.04767\n",
      "[ 4 ] iteration gradient norm is  1.5655928156881697e+19\n",
      "[ 4 ] conjugate gradient took  0  iteration(s).\n",
      "alpha is  26.999999999661185\n",
      "('Residual computation took', 8.58306884765625e-05, 'seconds')\n",
      "[ 4 ] iteration residual is  300165359533083.3\n",
      "[ 5 ] iteration gradient norm is  1.5655928629739764e+24\n",
      "[ 5 ] conjugate gradient took  0  iteration(s).\n",
      "alpha is  26.999999999999645\n",
      "('Residual computation took', 5.435943603515625e-05, 'seconds')\n",
      "[ 5 ] iteration residual is  3.001653595330759e+17\n",
      "[ 6 ] iteration gradient norm is  1.5655928634333315e+29\n",
      "[ 6 ] conjugate gradient took  0  iteration(s).\n",
      "alpha is  26.999999999999968\n",
      "('Residual computation took', 5.745887756347656e-05, 'seconds')\n",
      "[ 6 ] iteration residual is  3.0016535953307494e+20\n",
      "[ 7 ] iteration gradient norm is  1.5655928634377789e+34\n",
      "[ 7 ] conjugate gradient took  0  iteration(s).\n",
      "alpha is  26.999999999999922\n",
      "('Residual computation took', 4.9591064453125e-05, 'seconds')\n",
      "[ 7 ] iteration residual is  3.0016535953307256e+23\n",
      "[ 8 ] iteration gradient norm is  1.565592863437802e+39\n",
      "[ 8 ] conjugate gradient took  0  iteration(s).\n",
      "alpha is  26.999999999999954\n",
      "('Residual computation took', 4.887580871582031e-05, 'seconds')\n",
      "[ 8 ] iteration residual is  3.001653595330713e+26\n",
      "[ 9 ] iteration gradient norm is  1.5655928634377914e+44\n",
      "[ 9 ] conjugate gradient took  0  iteration(s).\n",
      "alpha is  27.000000000000018\n",
      "('Residual computation took', 4.839897155761719e-05, 'seconds')\n",
      "[ 9 ] iteration residual is  3.0016535953307176e+29\n",
      "Total number of CG iterations is  65\n"
     ]
    }
   ],
   "source": [
    "res = get_residual(tenpy,T,A)\n",
    "print(\"Start residual is \",res)\n",
    "x = flatten_A(A)\n",
    "print(\"x shape is\",x.shape)\n",
    "a = 0\n",
    "tol = 1e-4\n",
    "for i in range(iteration):\n",
    "    J = jacobian3(A)\n",
    "    JT = np.transpose(J)\n",
    "    H = np.dot(JT,J)\n",
    "    regu = 1/(i+1)\n",
    "    H += regu*np.identity(J.shape[1])\n",
    "    #H = fast_hessian3(A,regu)\n",
    "    \n",
    "    b = -1*gradient(T,A)\n",
    "    print(\"[\",i,\"] iteration gradient norm is \",la.norm(b))\n",
    "    #M = naive_diag_Neumann_preconditioner2(H)\n",
    "    M = naive_block_Jacobi(H,order,s*R)\n",
    "    #M = naive_block_diag_perturbative_preconditioner(H,order,s*R)\n",
    "    #M = naive_diag_perturbative_preconditioner(H)\n",
    "    #M = identity_like_iter_preconditioner2(H)\n",
    "    #M = Jacobi_preconditioner(H)\n",
    "    #f = F(T,A)\n",
    "    #b = np.dot(JT,f)\n",
    "    #L = la.cholesky(H)\n",
    "    #y = sla.solve_triangular(L,b,trans=0,lower=True)\n",
    "    #x = sla.solve_triangular(L,y,trans=1,lower=True)\n",
    "    #[dx,_] = spsalg.cg(H,b,x,1e-4,None,M)\n",
    "    #x,c = conjugate_gradient(H,x,b,tol,formula=\"PR\")\n",
    "    x,c = preconditioned_conjugate_gradient(H,x,b,M,tol,formula=\"PR\")\n",
    "    #x,c = preconditioned_conjugate_gradient2(H,x,b,M,tol)\n",
    "    print(\"[\",i,\"] conjugate gradient took \",c,\" iteration(s).\")\n",
    "    a += c\n",
    "    dA = form_dA(A,x)\n",
    "    R_tensor = compute_residual_tensor3(T,A)\n",
    "    alpha = compute_alpha_numerator(R_tensor,dA)/compute_alpha_denominator(dA)\n",
    "    print(\"alpha is \",alpha)\n",
    "    update_A(A,x,alpha)\n",
    "    #update_A(A,x)\n",
    "    res = get_residual(tenpy,T,A)\n",
    "    print(\"[\",i,\"] iteration residual is \",res)\n",
    "    if res<1e-4:\n",
    "        break\n",
    "print(\"Total number of CG iterations is \",a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast NLS Implementation with Tensor Contraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coefficient_matrix(G,n1,n2):\n",
    "    ret = np.ones(G[0].shape)\n",
    "    for i in range(len(G)):\n",
    "        if i!=n1 and i!=n2:\n",
    "            ret = np.einsum(\"ij,ij->ij\",ret,G[i])\n",
    "    return ret\n",
    "\n",
    "def compute_coefficient_matrix_matrix(G):\n",
    "    N = len(G)\n",
    "    ret = [[None for i in range(N)] for i in range(N)]\n",
    "    for n in range(N):\n",
    "        for p in range(n,N):\n",
    "            M = compute_coefficient_matrix(G,n,p)\n",
    "            ret[n][p] = M\n",
    "            if n!=p:\n",
    "                ret[p][n] = M\n",
    "    return ret\n",
    "\n",
    "def fast_hessian_contract(A,X,regu=1):\n",
    "    N = len(A)\n",
    "    ## Preprocessing step: should be moved outside of contraction \n",
    "    G = []\n",
    "    for mat in A:\n",
    "        G.append(mat.T.dot(mat))\n",
    "    \n",
    "    ret = []\n",
    "    for n in range(N):\n",
    "        for p in range(N):\n",
    "            ## Computation of M should be done outside of contraction\n",
    "            M = compute_coefficient_matrix(G,n,p)\n",
    "            if n==p:\n",
    "                Y = np.einsum(\"iz,zr->ir\",X[p],M)\n",
    "            else:\n",
    "                Y = np.einsum(\"iz,zr,jr,jz->ir\",A[n],M,A[p],X[p])\n",
    "            if p==0:\n",
    "                ret.append(Y)\n",
    "            else:\n",
    "                ret[n] += Y\n",
    "                \n",
    "    for i in range(N):\n",
    "        ret[i] += regu*X[i]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "X = [np.random.random((s,R)) for i in range(order)]\n",
    "x = flatten_A(X)\n",
    "J = jacobian3(A)\n",
    "JT = np.transpose(J)\n",
    "H = np.dot(JT,J)\n",
    "r1 = H.dot(x)\n",
    "r2 = flatten_A(fast_hessian_contract(A,X))\n",
    "print(np.isclose(r1,r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second-order Correction Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import scipy.linalg as sla\n",
    "import scipy.sparse.linalg as spsalg\n",
    "import tensors.synthetic_tensors as synthetic_tensors\n",
    "import backend.numpy_ext as tenpy\n",
    "from CPD.common_kernels import get_residual, compute_lin_sys, solve_sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_order_correction_sweep(T,A,dA,Regu):\n",
    "    G = compute_G(A)\n",
    "    TC = tenpy.einsum(\"ijk,ka->ija\",T,A[2])\n",
    "    M1 = tenpy.einsum(\"ija,ja->ia\",TC,A[1])\n",
    "    G1 = -1*M1 + np.dot(A[0],compute_lin_sys(tenpy,A[1],A[2],0))\n",
    "    RHS = -1*G1\n",
    "    for p in range(len(A)):\n",
    "        if p!=0:\n",
    "            M = compute_coefficient_matrix(G,0,p)\n",
    "            RHS -= np.einsum(\"iz,zr,jr,jz->ir\",A[0],M,A[p],dA[p])\n",
    "    #print(compute_lin_sys(tenpy,A[1],A[2],Regu))\n",
    "    dA[0] = solve_sys(tenpy,compute_lin_sys(tenpy,A[1],A[2],Regu), RHS)\n",
    "    A[0] += dA[0]\n",
    "    \n",
    "    G = compute_G(A)\n",
    "    M2 = tenpy.einsum(\"ija,ia->ja\",TC,A[0])\n",
    "    G2 = -1*M2 + np.dot(A[1],compute_lin_sys(tenpy,A[0],A[2],0))\n",
    "    RHS = -1*G2\n",
    "    for p in range(len(A)):\n",
    "        if p!=1:\n",
    "            M = compute_coefficient_matrix(G,1,p)\n",
    "            RHS -= np.einsum(\"iz,zr,jr,jz->ir\",A[1],M,A[p],dA[p])\n",
    "    dA[1] = solve_sys(tenpy,compute_lin_sys(tenpy,A[0],A[2],Regu), RHS)\n",
    "    A[1] += dA[1]\n",
    "    \n",
    "    G = compute_G(A)\n",
    "    M3 = tenpy.einsum(\"ijk,ia,ja->ka\",T,A[0],A[1])\n",
    "    G3 = -1*M3 + np.dot(A[2],compute_lin_sys(tenpy,A[0],A[1],0))\n",
    "    RHS = -1*G3\n",
    "    for p in range(len(A)):\n",
    "        if p!=2:\n",
    "            M = compute_coefficient_matrix(G,2,p)\n",
    "            RHS -= np.einsum(\"iz,zr,jr,jz->ir\",A[2],M,A[p],dA[p])\n",
    "    dA[2] = solve_sys(tenpy,compute_lin_sys(tenpy,A[0],A[1],Regu), RHS)\n",
    "    A[2] += dA[2]\n",
    "    \n",
    "def compute_coefficient_matrix(G,n1,n2):\n",
    "    ret = np.ones(G[0].shape)\n",
    "    for i in range(len(G)):\n",
    "        if i!=n1 and i!=n2:\n",
    "            ret = np.einsum(\"ij,ij->ij\",ret,G[i])\n",
    "    return ret\n",
    "    \n",
    "def compute_G(A):\n",
    "    G = []\n",
    "    for i in range(len(A)):\n",
    "        G.append(np.einsum(\"ij,ik->jk\",A[i],A[i]))\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Residual computation took', 8.153915405273438e-05, 'seconds')\n",
      "[ 0 ] iteration residual is  2.1821606211685447\n",
      "('Residual computation took', 6.508827209472656e-05, 'seconds')\n",
      "[ 1 ] iteration residual is  1.5412114517943332\n",
      "('Residual computation took', 5.2928924560546875e-05, 'seconds')\n",
      "[ 2 ] iteration residual is  1.2540687403959137\n",
      "('Residual computation took', 5.0067901611328125e-05, 'seconds')\n",
      "[ 3 ] iteration residual is  0.9707816022067516\n",
      "('Residual computation took', 5.030632019042969e-05, 'seconds')\n",
      "[ 4 ] iteration residual is  0.8305861270741097\n",
      "('Residual computation took', 4.9591064453125e-05, 'seconds')\n",
      "[ 5 ] iteration residual is  0.655560646941938\n",
      "('Residual computation took', 4.744529724121094e-05, 'seconds')\n",
      "[ 6 ] iteration residual is  0.9068039361853778\n",
      "('Residual computation took', 5.888938903808594e-05, 'seconds')\n",
      "[ 7 ] iteration residual is  1.5502071207719943\n",
      "('Residual computation took', 5.269050598144531e-05, 'seconds')\n",
      "[ 8 ] iteration residual is  0.9762887002240904\n",
      "('Residual computation took', 4.887580871582031e-05, 'seconds')\n",
      "[ 9 ] iteration residual is  2.556461984527951\n",
      "('Residual computation took', 4.696846008300781e-05, 'seconds')\n",
      "[ 10 ] iteration residual is  3.308781579594584\n",
      "('Residual computation took', 5.793571472167969e-05, 'seconds')\n",
      "[ 11 ] iteration residual is  2.1332805984479233\n",
      "('Residual computation took', 6.389617919921875e-05, 'seconds')\n",
      "[ 12 ] iteration residual is  8.155430115508851\n",
      "('Residual computation took', 5.459785461425781e-05, 'seconds')\n",
      "[ 13 ] iteration residual is  63.96604906405404\n",
      "('Residual computation took', 5.364418029785156e-05, 'seconds')\n",
      "[ 14 ] iteration residual is  1578.9528969555902\n",
      "('Residual computation took', 7.796287536621094e-05, 'seconds')\n",
      "[ 15 ] iteration residual is  32251.772288498945\n",
      "('Residual computation took', 5.53131103515625e-05, 'seconds')\n",
      "[ 16 ] iteration residual is  657742.8807289782\n",
      "('Residual computation took', 5.364418029785156e-05, 'seconds')\n",
      "[ 17 ] iteration residual is  13412557.40215621\n",
      "('Residual computation took', 5.269050598144531e-05, 'seconds')\n",
      "[ 18 ] iteration residual is  273512809.0301353\n",
      "('Residual computation took', 5.364418029785156e-05, 'seconds')\n",
      "[ 19 ] iteration residual is  5577556451.801562\n",
      "('Residual computation took', 5.555152893066406e-05, 'seconds')\n",
      "[ 20 ] iteration residual is  113739231661.50833\n",
      "('Residual computation took', 5.8650970458984375e-05, 'seconds')\n",
      "[ 21 ] iteration residual is  2319405085019.546\n",
      "('Residual computation took', 5.2928924560546875e-05, 'seconds')\n",
      "[ 22 ] iteration residual is  47298015553974.0\n",
      "('Residual computation took', 5.888938903808594e-05, 'seconds')\n",
      "[ 23 ] iteration residual is  964515551759484.0\n",
      "('Residual computation took', 5.269050598144531e-05, 'seconds')\n",
      "[ 24 ] iteration residual is  1.9668695159621212e+16\n",
      "('Residual computation took', 5.91278076171875e-05, 'seconds')\n",
      "[ 25 ] iteration residual is  4.010900276065e+17\n",
      "('Residual computation took', 5.4836273193359375e-05, 'seconds')\n",
      "[ 26 ] iteration residual is  8.179150113406969e+18\n",
      "('Residual computation took', 4.863739013671875e-05, 'seconds')\n",
      "[ 27 ] iteration residual is  1.6679172248899125e+20\n",
      "('Residual computation took', 5.745887756347656e-05, 'seconds')\n",
      "[ 28 ] iteration residual is  3.4012676506870853e+21\n",
      "('Residual computation took', 5.650520324707031e-05, 'seconds')\n",
      "[ 29 ] iteration residual is  6.935968679365415e+22\n",
      "('Residual computation took', 5.817413330078125e-05, 'seconds')\n",
      "[ 30 ] iteration residual is  1.4144038770785798e+24\n",
      "('Residual computation took', 5.626678466796875e-05, 'seconds')\n",
      "[ 31 ] iteration residual is  2.884295503592083e+25\n",
      "('Residual computation took', 5.841255187988281e-05, 'seconds')\n",
      "[ 32 ] iteration residual is  5.8817433173504536e+26\n",
      "('Residual computation took', 5.221366882324219e-05, 'seconds')\n",
      "[ 33 ] iteration residual is  1.1994230274988262e+28\n",
      "('Residual computation took', 5.841255187988281e-05, 'seconds')\n",
      "[ 34 ] iteration residual is  2.4459000015364568e+29\n",
      "('Residual computation took', 5.245208740234375e-05, 'seconds')\n",
      "[ 35 ] iteration residual is  4.9877538452728203e+30\n",
      "('Residual computation took', 5.936622619628906e-05, 'seconds')\n",
      "[ 36 ] iteration residual is  1.0171179690668558e+32\n",
      "('Residual computation took', 6.270408630371094e-05, 'seconds')\n",
      "[ 37 ] iteration residual is  2.0741379688958944e+33\n",
      "('Residual computation took', 5.817413330078125e-05, 'seconds')\n",
      "[ 38 ] iteration residual is  4.229645375317136e+34\n",
      "('Residual computation took', 5.3882598876953125e-05, 'seconds')\n",
      "[ 39 ] iteration residual is  8.625221788145942e+35\n",
      "('Residual computation took', 5.888938903808594e-05, 'seconds')\n",
      "[ 40 ] iteration residual is  1.7588815206317251e+37\n",
      "('Residual computation took', 5.602836608886719e-05, 'seconds')\n",
      "[ 41 ] iteration residual is  3.5867648155686166e+38\n",
      "('Residual computation took', 5.9604644775390625e-05, 'seconds')\n",
      "[ 42 ] iteration residual is  7.314240152787773e+39\n",
      "('Residual computation took', 5.269050598144531e-05, 'seconds')\n",
      "[ 43 ] iteration residual is  1.4915421490821038e+41\n",
      "('Residual computation took', 5.9604644775390625e-05, 'seconds')\n",
      "[ 44 ] iteration residual is  3.0415982193865087e+42\n",
      "('Residual computation took', 5.173683166503906e-05, 'seconds')\n",
      "[ 45 ] iteration residual is  6.202519810699584e+43\n",
      "('Residual computation took', 5.8650970458984375e-05, 'seconds')\n",
      "[ 46 ] iteration residual is  1.2648367478950085e+45\n",
      "('Residual computation took', 5.173683166503906e-05, 'seconds')\n",
      "[ 47 ] iteration residual is  2.5792936542756146e+46\n",
      "('Residual computation took', 5.602836608886719e-05, 'seconds')\n",
      "[ 48 ] iteration residual is  5.2597742483828286e+47\n",
      "('Residual computation took', 5.221366882324219e-05, 'seconds')\n",
      "[ 49 ] iteration residual is  1.0725891989107706e+49\n"
     ]
    }
   ],
   "source": [
    "Regu = 1\n",
    "for i in range(iteration):\n",
    "    second_order_correction_sweep(T,A,dA,Regu)\n",
    "    res = get_residual(tenpy,T,A)\n",
    "    print(\"[\",i,\"] iteration residual is \",res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
